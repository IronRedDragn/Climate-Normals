{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782d5b99",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook illustrates the steps I undertook to obtain, format, and store 9 different climate normal datasets onto a Postgresql database.\n",
    "\n",
    "These climate normals were calculated by the National Centers for Environmental Information (NCEI) for weather stations located across the United States. Climate Normals act as a way to compare present weather and future forecasts to climatological events. The normals are generated every 10 years for each of the previous 30 year period. The climate normals time periods are the 1981-2010, 1991-2020, and the 15-year period climate normal 2006-2020.\n",
    "\n",
    "For each of these time periods, hourly, daily, and monthly datasets were generated. In total, 9 different datasets were obtained and stored into a Postgresql database.\n",
    "\n",
    "The climate normals can be located at the following links:\n",
    "\n",
    "* 1981-2010\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1981-2010/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1981-2010/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1981-2010/archive/)\n",
    "    \n",
    "* 1991-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1991-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1991-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1991-2020/archive/)\n",
    "    \n",
    "* 2006-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/2006-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/2006-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/2006-2020/archive/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa25044e-be81-46e7-9a69-1073cf57b053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T13:37:02.584862Z",
     "start_time": "2021-07-28T13:37:02.576930Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "import psycopg2 as psql\n",
    "\n",
    "import climate_normal_scripts as cns\n",
    "importlib.reload(cns) # reloads script if script was changed after notebook kernel started\n",
    "\n",
    "dataFolder = cns.dataFolder\n",
    "txtFiles_Folder = cns.txtFiles_Folder\n",
    "# normals_dict = cns.normals_dict\n",
    "normals_dict = {'1981-2010': {'normals-hourly': { },'normals-daily': { }, 'normals-monthly': { }},\n",
    "                '1991-2020': {'normals-hourly': { },'normals-daily': { }, 'normals-monthly': { }},\n",
    "                '2006-2020': {'normals-hourly': { },'normals-daily': { }, 'normals-monthly': { }},\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6c4374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T13:33:15.141232Z",
     "start_time": "2021-07-28T13:33:15.137808Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_Files(combined_file, files_toCombine):\n",
    "    if os.path.exists(combined_file):\n",
    "        return print(f'{combined_file} exists...')\n",
    "    \n",
    "    with open(combined_file, 'w') as outfile:\n",
    "        initial = 1\n",
    "        for file in files_toCombine:\n",
    "            with open(file, 'r') as infile:\n",
    "                header = infile.readline()\n",
    "                if initial:\n",
    "                    outfile.write(f'{header}')\n",
    "                    initial = 0\n",
    "                outfile.write(infile.read())\n",
    "    return print(f'{combined_file} created...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20dddc34-1e3e-44f6-a355-46d6f3b846c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T13:37:05.114453Z",
     "start_time": "2021-07-28T13:37:05.001966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 1981-2010 normals...\n",
      "1981-2010 normals-hourly already downloaded...\n",
      "Getting 1981-2010 normals...\n",
      "1981-2010 normals-daily already downloaded...\n",
      "Getting 1981-2010 normals...\n",
      "1981-2010 normals-monthly already downloaded...\n",
      "Getting 1991-2020 normals...\n",
      "1991-2020 normals-hourly already downloaded...\n",
      "Getting 1991-2020 normals...\n",
      "1991-2020 normals-daily already downloaded...\n",
      "Getting 1991-2020 normals...\n",
      "1991-2020 normals-monthly already downloaded...\n",
      "Getting 2006-2020 normals...\n",
      "2006-2020 normals-hourly already downloaded...\n",
      "Getting 2006-2020 normals...\n",
      "2006-2020 normals-daily already downloaded...\n",
      "Getting 2006-2020 normals...\n",
      "2006-2020 normals-monthly already downloaded...\n",
      "Headers files generated at ./txt_files/csv_headers/\n"
     ]
    }
   ],
   "source": [
    "# get climate normals files\n",
    "normal_periods = ['1981-2010','1991-2020','2006-2020']\n",
    "normal_types = ['normals-hourly','normals-daily', 'normals-monthly']\n",
    "main_url = 'https://www.ncei.noaa.gov/data/'\n",
    "\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        files_location = f'{dataFolder}{normal_period}/{normal_type}/'\n",
    "        cns.get_climate_normals(main_url, normal_type, normal_period, files_location)\n",
    "        normals_dict[normal_period][normal_type] = {'files_location': files_location,\n",
    "                                                    'inventory_file': f'{txtFiles_Folder}station_inventory/station-inventory-{normal_period}_{normal_type}.csv',\n",
    "                                                    'variables_file': f'{txtFiles_Folder}variables/variables-{normal_period}-{normal_type}.csv'}\n",
    "\n",
    "cns.generate_header_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94dca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(df, normal_type):\n",
    "    '''\n",
    "    Function which reads in the DATE field and compares to the expected Full time range. If there are any missing time records, those missing times are added\n",
    "    into the record, with nan values for the corresponding weather climate normals for those missing times. Populates the month,day,hour fields based off the new DATE field. \n",
    "\n",
    "    Years are dummy values. For Daily files, the year is set to a leap year to also include the Leap day. Leap day is needed in hours and months files only.\n",
    "    '''\n",
    "\n",
    "    if normal_type == 'normals-hourly':\n",
    "        full_range = pd.date_range(start= '1900-01-01 00:00:00', end = '1900-12-31 23:00:00', freq = 'H') # timeindex for all hours \n",
    "        dateOUT_format = '%b-%d %H:%M'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m-%dT%X')                                            # reformats DATE column into timeindex for comparison\n",
    "        \n",
    "    elif normal_type == 'normals-daily':\n",
    "        full_range = pd.date_range(start= '1904-01-01', end = '1904-12-31', freq = 'D')\n",
    "        dateOUT_format = '%b-%d'\n",
    "        df.DATE = pd.to_datetime(df.DATE + '-1904', format = '%m-%d-%Y')                                  # reformats DATE column, adds year column to avoid error for leap year\n",
    "    \n",
    "    elif normal_type == 'normals-monthly':\n",
    "        full_range = pd.date_range(start= '1900-01', end = '1900-12', freq = 'MS')\n",
    "        dateOUT_format = '%b'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m')                                                  # reformats DATE column\n",
    "\n",
    "    df = df.set_index('DATE').reindex(full_range)                                                     # sets DATE column to index then adds rows to record based off missing dates\n",
    "    df.reset_index(inplace=True,drop=True)                                                            # resets index back to default dropping old DATE column\n",
    "    df['DATE'] = full_range.strftime(dateOUT_format)\n",
    "    \n",
    "    df.month = full_range.month\n",
    "    df.day = full_range.day\n",
    "    df.hour = full_range.hour\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_variables(df, headers):\n",
    "    tenths = [header.split(',')[0] for header in headers if header.split(',')[1] == 'Tenths']\n",
    "    hundredths = [header.split(',')[0] for header in headers if header.split(',')[1] == 'Hundredths']\n",
    "    wind_dir = [header.split(',')[0] for header in headers if header.split(',')[1] == 'Wind_Direction']\n",
    "    wind_dir_labels = {1.0:'N', 2.0:'NE', 3.0:'E', 4.0:'SE', 5.0: 'S', 6.0:'SW', 7.0:'W', 8.0:'NW'} \n",
    "\n",
    "    df[tenths] = df[tenths].divide(10)\n",
    "    df[hundredths] = df[hundredths].divide(100)\n",
    "    df[wind_dir] = df[wind_dir].replace(wind_dir_labels)\n",
    "    return df\n",
    "\n",
    "def format_file(raw_file,headers,normal_type):\n",
    "    formatted_File = f'{files_location}formatted_files/{raw_file.name}'\n",
    "    if not os.path.exists(formatted_File):\n",
    "        df = pd.read_csv(raw_file)        \n",
    "\n",
    "        colsName = [x.split(',')[0] for x in headers]\n",
    "            \n",
    "            ## standardize headers: adds missing columns(if any) and reorders columns based off headers\n",
    "        df = df.reindex(columns=colsName) \n",
    "        df = df.replace(to_replace=[-9999.0, -7777.0, -6666.0, -4444.0, ' '], value= np.nan) \n",
    "        df = format_date(df, normal_type)\n",
    "        df = format_variables(df, headers[5:])\n",
    "        df = df.reindex(columns=colsName) # ensures columns in correct order after formatting file\n",
    "        df = df.replace(to_replace=[' ',''], value= np.nan) \n",
    "        df = df.fillna(value= {'STATION': df.STATION.mode()[0]}) # station metadata to replace NaN in the metadata column\n",
    "        df.to_csv(formatted_File,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b240a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# standardizing normal files\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        headerFile = f'{txtFiles_Folder}csv_headers/headers-{normal_period}-{normal_type}.txt'\n",
    "        normals_dict[normal_period][normal_type]['headerFile'] = headerFile\n",
    "        headers = []\n",
    "        with open(headerFile, 'r') as hfile:\n",
    "            lines = hfile.readlines()\n",
    "            for line in lines:\n",
    "                headers.append(line.strip('\\n'))\n",
    "\n",
    "        files_location = f'{normals_dict[normal_period][normal_type][\"files_location\"]}'    \n",
    "        if not os.path.exists(f'{files_location}formatted_files/'):\n",
    "                os.makedirs(f'{files_location}formatted_files/')\n",
    "\n",
    "        print(f'Reformatting files: {normal_period} - {normal_type}...')\n",
    "        normal_files = list(Path(f'{files_location}station_files/').glob('*.csv'))\n",
    "        normals_dict[normal_period][normal_type]['Total_Stations'] = len(normal_files)\n",
    "\n",
    "        part_format = partial(format_file, headers = headers, normal_type = normal_type) # make into partial function to be applied to map \n",
    "        pool = multiprocessing.Pool()\n",
    "        pool.map(part_format, normal_files)\n",
    "        pool.close()\n",
    "        \n",
    "        # print(f'{normal_period} - {normal_type} files formatted...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adc5bc",
   "metadata": {},
   "source": [
    "Now to combine all the normal files together for the same type and same time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        files_location = f'{normals_dict[normal_period][normal_type][\"files_location\"]}'\n",
    "        files_toCombine = Path(f'{files_location}formatted_files').glob('*.csv')\n",
    "        combined_file = f'{files_location}combined_{normal_period}-{normal_type}.csv'\n",
    "        combine_Files(combined_file, files_toCombine)\n",
    "        normals_dict[normal_period][normal_type]['combined_file'] = combined_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f28eb4",
   "metadata": {},
   "source": [
    "Connect to postgres server to create a database for the climate normals: climate_normals_db. \n",
    "\n",
    "The tables will be:\n",
    "- states\n",
    "- HSI_1981_2010      : 1981-2010 hourly station inventory\n",
    "- HVAR_1981_2010     : 1981-2010 hourly variables : \n",
    "- HNORMALS_1981_2010 : 1981-2010 hourly normals\n",
    "- DSI_1981_2010      : 1981-2010 daily station inventory\n",
    "- DVAR_1981_2010     : 1981-2010 daily variables\n",
    "- DNORMALS_1981_2010 : 1981-2010 daily normals\n",
    "- MSI_1981_2010      : 1981-2010 monthly station inventory\n",
    "- MVAR_1981_2010     : 1981-2010 monthly variables\n",
    "- MNORMALS_1981_2010 : 1981-2010 monthly normals\n",
    "- HSI_1991_2020      : 1991-2020 hourly station inventory\n",
    "- HVAR_1991_2020     : 1991-2020 hourly variables\n",
    "- HNORMALS_1991_2020 : 1991-2020 hourly normals\n",
    "- DSI_1991_2020      : 1991-2020 daily station inventory\n",
    "- DVAR_1991_2020     : 1991-2020 daily variables\n",
    "- DNORMALS_1991_2020 : 1991-2020 daily normals\n",
    "- MSI_1991_2020      : 1991-2020 monthly station inventory\n",
    "- MVAR_1991_2020     : 1991-2020 monthly variables\n",
    "- MNORMALS_1991_2020 : 1991-2020 monthly normals\n",
    "- HSI_2006_2020      : 2006-2020 hourly station inventory\n",
    "- HVAR_2006_2020     : 2006-2020 hourly variables\n",
    "- HNORMALS_2006_2020 : 2006-2020 hourly normals\n",
    "- DSI_2006_2020      : 2006-2020 daily station inventory\n",
    "- DVAR_2006_2020     : 2006-2020 daily variables\n",
    "- DNORMALS_2006_2020 : 2006-2020 daily normals\n",
    "- MSI_2006_2020      : 2006-2020 monthly station inventory\n",
    "- MVAR_2006_2020     : 2006-2020 monthly variables\n",
    "- MNORMALS_2006_2020 : 2006-2020 monthly normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "def config(filename='./sql/database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    "\n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "    return db\n",
    "\n",
    "def connect_db(db_Name):\n",
    "    \"\"\" Connect to the database\"\"\"\n",
    "    try:\n",
    "        # read connection parameters\n",
    "        params = config()\n",
    "\n",
    "        # connect to the PostgreSQL server\n",
    "        print(f'Connecting to the {db_Name} database...')\n",
    "        conn = psql.connect(database=db_Name, **params)\n",
    "\n",
    "    except psql.OperationalError as e:\n",
    "        print(f'There is no database named {db_Name}...')\n",
    "        create_db(db_Name)\n",
    "        conn.close()\n",
    "        return connect_db(db_Name)\n",
    "    else:\n",
    "        print(f'***Connected: {db_Name}***')\n",
    "        return conn\n",
    "        \n",
    "def create_db(db_Name):\n",
    "    # read connection parameters\n",
    "    params = config()\n",
    "    print(f'Connecting to postgres database to create new database:{db_Name}')\n",
    "    conn = psql.connect(database='postgres',**params)\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    sql=f'CREATE database {db_Name}'\n",
    "\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        cursor.close()\n",
    "    except psql.errors.lookup('42P04'): #psql error code for duplicatedatabase\n",
    "        conn.close()\n",
    "        cursor.close()\n",
    "        return print(f\"Database: [{db_Name}] already exists....\")\n",
    "    else:\n",
    "        conn.close()\n",
    "        cursor.close()\n",
    "        return print(f\"***Database: {db_Name} created successfully***\")\n",
    "\n",
    "def execute_query(connection, query):\n",
    "    connection.autocommit = True\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        print(\"Query executed successfully\")\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "\n",
    "def execute_read_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f112e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_state_table = \"\"\"\n",
    "DROP TABLE states;\n",
    "CREATE TABLE IF NOT EXISTS states(\n",
    "    statelong TEXT NOT NULL,\n",
    "    stateabbr CHAR(2) PRIMARY KEY\n",
    ")\n",
    "\"\"\"\n",
    "execute_query(conn, create_state_table)\n",
    "\n",
    "cur = conn.cursor()\n",
    "with open(f'{txtFiles_Folder}states.csv', 'r') as csv:\n",
    "    next(csv) # skips header\n",
    "    cur.copy_from(csv, 'states', sep=',')\n",
    "\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of variable tables\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        if normal_type == 'normals-hourly': temp = 'hly'\n",
    "        elif normal_type == 'normals-daily': temp = 'dly'\n",
    "        elif normal_type == 'normals-monthly': temp = 'mly'      \n",
    "        tableName = f'var_{temp}_{normal_period}'.replace('-','_')   # Replaces - with _ to play nicer with postgres. \n",
    "\n",
    "        create_variable_table = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {tableName};\n",
    "        CREATE TABLE IF NOT EXISTS {tableName}(\n",
    "            variable TEXT PRIMARY KEY,\n",
    "            description TEXT NOT NULL,\n",
    "            units TEXT NOT NULL\n",
    "        )\n",
    "        \"\"\"\n",
    "        execute_query(conn, create_variable_table)\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "        with open(normals_dict[normal_period][normal_type]['variables_file'], 'r') as csv:\n",
    "            cur.copy_from(csv, tableName, sep=',')\n",
    "        conn.commit()\n",
    "        cur.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of station inventory tables\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        if normal_type == 'normals-hourly': temp = 'hly'\n",
    "        elif normal_type == 'normals-daily': temp = 'dly'\n",
    "        elif normal_type == 'normals-monthly': temp = 'mly'        \n",
    "        tableName = f'si_{temp}_{normal_period}'.replace('-','_')\n",
    "        \n",
    "        # create_SI_tables = f\"\"\"\n",
    "        # DROP TABLE IF EXISTS {tableName};\"\"\"\n",
    "        # execute_query(conn, create_SI_tables)\n",
    "\n",
    "        create_SI_tables = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {tableName} CASCADE;\n",
    "        CREATE TABLE IF NOT EXISTS {tableName}\n",
    "        (\n",
    "            \"stationID\" TEXT PRIMARY KEY,\n",
    "            latitude  NUMERIC NOT NULL,\n",
    "            longitude NUMERIC NOT NULL,\n",
    "            elevation NUMERIC NOT NULL,\n",
    "            state CHAR(2) NOT NULL,\n",
    "            \"stationName\" TEXT NOT NULL,\n",
    "            network TEXT,\n",
    "            \"wmoID\" TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # execute_query(conn, create_SI_tables)\n",
    "        # cur = conn.cursor()\n",
    "        # with open(normals_dict[normal_period][normal_type]['inventory_file'], 'r') as csv:\n",
    "        #     next(csv) #skips header\n",
    "        #     cur.copy_from(csv, tableName, sep=',')\n",
    "\n",
    "        # conn.commit()\n",
    "\n",
    "        # alter_SI_tables = f\"\"\"\n",
    "        # ALTER TABLE {tableName} ADD COLUMN coordinates GEOMETRY(POINT, 4326);\n",
    "        # UPDATE {tableName} SET coordinates = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326);\n",
    "        # \"\"\"\n",
    "        # execute_query(conn, alter_SI_tables)\n",
    "        # cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ff7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of normals tables\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        if normal_type == 'normals-hourly': temp = 'hly'\n",
    "        elif normal_type == 'normals-daily': temp = 'dly'\n",
    "        elif normal_type == 'normals-monthly': temp = 'mly'        \n",
    "        tableName = f'normals_{temp}_{normal_period}'.replace('-','_')\n",
    "\n",
    "        normals_list = '\"stationID\" TEXT NOT NULL, date TEXT NOT NULL, month SMALLINT NOT NULL, day SMALLINT NOT NULL, hour SMALLINT NOT NULL,'\n",
    "\n",
    "        with open(normals_dict[normal_period][normal_type]['headerFile'], 'r') as header:\n",
    "            lines = header.readlines()[5:]\n",
    "\n",
    "        for count, line in enumerate(lines):\n",
    "            var = line.strip('\\n').split(',')[0]\n",
    "            if '1STDIR' in var or '2NDDIR' in var:\n",
    "                normals_list += f'\"{var}\" TEXT,'\n",
    "                continue\n",
    "\n",
    "            if normal_period == '1981-2010':\n",
    "                if count % 2 == 0:\n",
    "                    normals_list += f'\"{var}\" NUMERIC,'\n",
    "                else:\n",
    "                    normals_list += f'\"{var}\" TEXT,'\n",
    "                continue\n",
    "\n",
    "            if count % 4 == 0 or count % 4 == 3:\n",
    "                normals_list += f'\"{var}\" NUMERIC,'\n",
    "            else:\n",
    "                normals_list += f'\"{var}\" TEXT,'\n",
    "                \n",
    "        normals_list = normals_list.rstrip(',')\n",
    "            \n",
    "        parentTable = f'si_{temp}_{normal_period}'.replace('-','_')\n",
    "        create_normals_tables = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {tableName};\n",
    "        CREATE TABLE IF NOT EXISTS {tableName}\n",
    "        (\n",
    "            {normals_list},\n",
    "            CONSTRAINT \"fk_stationID\"\n",
    "                FOREIGN KEY(\"stationID\")\n",
    "                    REFERENCES {parentTable}(\"stationID\")\n",
    "                    ON DELETE CASCADE\n",
    "        )\n",
    "        \"\"\"\n",
    "        # execute_query(conn, create_normals_tables)\n",
    "\n",
    "        # cur = conn.cursor()\n",
    "        # with open(normals_dict[normal_period][normal_type]['combined_file'], 'r') as csv:\n",
    "        #     next(csv) #skips header\n",
    "        #     cur.copy_from(csv, tableName, sep=',', null=\"\")\n",
    "        # print(f'{tableName} created and filled...')\n",
    "        # conn.commit()\n",
    "        # cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf44039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125000e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normals_dict[normal_period][normal_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b21ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8829f2f28fb4fe9df99e8c305ffe937ba372ff45bcdf063cdbd318f5788220f1"
  },
  "kernelspec": {
   "display_name": "Climate-Normals",
   "language": "python",
   "name": "climate-normals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
