{
 "cells": [
  {
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook illustrates the steps I undertook to obtain, format, and store 9 different climate normal datasets onto a Postgresql database.\n",
    "\n",
    "These climate normals were calculated by the National Centers for Environmental Information (NCEI) for weather stations located across the United States. Climate Normals act as a way to compare present weather and future forecasts to climatological events. The normals are generated every 10 years for each of the previous 30 year period. The climate normals time periods are the 1981-2010, 1991-2020, and the 15-year period climate normal 2006-2020.\n",
    "\n",
    "For each of these time periods, hourly, daily, and monthly datasets were generated. In total, 9 different datasets were obtained and stored into a Postgresql database.\n",
    "\n",
    "The climate normals can be located at the following links:\n",
    "\n",
    "* 1981-2010\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1981-2010/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1981-2010/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1981-2010/archive/)\n",
    "    \n",
    "* 1991-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1991-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1991-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1991-2020/archive/)\n",
    "    \n",
    "* 2006-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/2006-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/2006-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/2006-2020/archive/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa25044e-be81-46e7-9a69-1073cf57b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "import psycopg2 as psql\n",
    "\n",
    "import climate_normal_scripts as cns\n",
    "importlib.reload(cns) # reloads script if script was changed after notebook kernel started\n",
    "\n",
    "dataFolder = cns.dataFolder\n",
    "txtFiles_Folder = cns.txtFiles_Folder\n",
    "normals_dict = cns.normals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_Files(combined_file, files_toCombine):\n",
    "    if os.path.exists(combined_file):\n",
    "        return print(f'{combined_file} exists...')\n",
    "    \n",
    "    with open(combined_file, 'w') as outfile:\n",
    "        initial = 1\n",
    "        for file in files_toCombine:\n",
    "            with open(file, 'r') as infile:\n",
    "                header = infile.readline()\n",
    "                if initial:\n",
    "                    outfile.write(f'{header}')\n",
    "                    initial = 0\n",
    "                outfile.write(infile.read())\n",
    "    return print(f'{combined_file} created...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20dddc34-1e3e-44f6-a355-46d6f3b846c0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1981-2010 normals-hourly already downloaded...\n1981-2010 normals-daily already downloaded...\n1981-2010 normals-monthly already downloaded...\n1991-2020 normals-hourly already downloaded...\n1991-2020 normals-daily already downloaded...\n1991-2020 normals-monthly already downloaded...\n2006-2020 normals-hourly already downloaded...\n2006-2020 normals-daily already downloaded...\n2006-2020 normals-monthly already downloaded...\nHeaders files generated at ./txt_files/csv_headers/\n"
     ]
    }
   ],
   "source": [
    "# get climate normals files\n",
    "normal_periods = ['1981-2010','1991-2020','2006-2020']\n",
    "normal_types = ['normals-hourly','normals-daily', 'normals-monthly']\n",
    "main_url = 'https://www.ncei.noaa.gov/data/'\n",
    "\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        files_location = f'{dataFolder}{normal_period}/{normal_type}/'\n",
    "        cns.get_climate_normals(main_url, normal_type, normal_period, files_location)\n",
    "        normals_dict[normal_period][normal_type] = {'files_location': files_location,\n",
    "                                                    'inventory_file': f'{txtFiles_Folder}station_inventory/station-inventory-{normal_period}_{normal_type}.csv',\n",
    "                                                    'variables_file': f'{txtFiles_Folder}variables/variables-{normal_period}-{normal_type}.csv'}\n",
    "\n",
    "cns.generate_header_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(df, normal_type):\n",
    "    '''\n",
    "    Function which reads in the DATE field and compares to the expected Full time range. If there are any missing time records, those missing times are added\n",
    "    into the record, with nan values for the corresponding weather climate normals for those missing times. Populates the month,day,hour fields based off the new DATE field. \n",
    "\n",
    "    Years are dummy values. For Daily files, the year is set to a leap year to also include the Leap day. Leap day is needed in hours and months files only.\n",
    "    '''\n",
    "\n",
    "    if normal_type == 'normals-hourly':\n",
    "        full_range = pd.date_range(start= '1900-01-01 00:00:00', end = '1900-12-31 23:00:00', freq = 'H') # timeindex for all hours \n",
    "        dateOUT_format = '%b-%d %H:%M'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m-%dT%X')                                            # reformats DATE column into timeindex for comparison\n",
    "        \n",
    "    elif normal_type == 'normals-daily':\n",
    "        full_range = pd.date_range(start= '1904-01-01', end = '1904-12-31', freq = 'D')\n",
    "        dateOUT_format = '%b-%d'\n",
    "        df.DATE = pd.to_datetime(df.DATE + '-1904', format = '%m-%d-%Y')                                  # reformats DATE column, adds year column to avoid error for leap year\n",
    "    \n",
    "    elif normal_type == 'normals-monthly':\n",
    "        full_range = pd.date_range(start= '1900-01', end = '1900-12', freq = 'MS')\n",
    "        dateOUT_format = '%b'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m')                                                  # reformats DATE column\n",
    "\n",
    "    df = df.set_index('DATE').reindex(full_range)                                                     # sets DATE column to index then adds rows to record based off missing dates\n",
    "    df.reset_index(inplace=True,drop=True)                                                            # resets index back to default dropping old DATE column\n",
    "    df['DATE'] = full_range.strftime(dateOUT_format)\n",
    "    \n",
    "    df.month = full_range.month\n",
    "    df.day = full_range.day\n",
    "    df.hour = full_range.hour\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_variables(df, headers):\n",
    "    tenths = [header.split(',')[0] for header in headers if header.split(',')[1] == 'Tenths']\n",
    "    hundredths = [header.split(',')[0] for header in headers if header.split(',')[1] == 'Hundredths']\n",
    "    wind_dir = [header.split(',')[0] for header in headers if header.split(',')[1] == 'Wind_Direction']\n",
    "    wind_dir_labels = {1.0:'N', 2.0:'NE', 3.0:'E', 4.0:'SE', 5.0: 'S', 6.0:'SW', 7.0:'W', 8.0:'NW'} \n",
    "\n",
    "    df[tenths] = df[tenths].divide(10)\n",
    "    df[hundredths] = df[hundredths].divide(100)\n",
    "    df[wind_dir] = df[wind_dir].replace(wind_dir_labels)\n",
    "    return df\n",
    "\n",
    "def format_file(raw_file,headers,normal_type):\n",
    "    formatted_File = f'{files_location}formatted_files/{raw_file.name}'\n",
    "    if not os.path.exists(formatted_File):\n",
    "        df = pd.read_csv(raw_file)        \n",
    "\n",
    "        colsName = [x.split(',')[0] for x in headers]\n",
    "            \n",
    "            ## standardize headers: adds missing columns(if any) and reorders columns based off headers\n",
    "        df = df.reindex(columns=colsName) \n",
    "        df = df.replace(to_replace=[-9999.0, -7777.0, -6666.0, -4444.0, ' '], value= np.nan) \n",
    "        df = format_date(df, normal_type)\n",
    "        df = format_variables(df, headers[5:])\n",
    "        df = df.reindex(columns=colsName) # ensures columns in correct order after formatting file\n",
    "        df = df.replace(to_replace=[' ',''], value= np.nan) \n",
    "        df = df.fillna(value= {'STATION': df.STATION.mode()[0]}) # station metadata to replace NaN in the metadata column\n",
    "        df.to_csv(formatted_File,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reformatting files: 1981-2010 - normals-hourly...\n",
      "Reformatting files: 1981-2010 - normals-daily...\n",
      "Reformatting files: 1981-2010 - normals-monthly...\n",
      "Reformatting files: 1991-2020 - normals-hourly...\n",
      "Reformatting files: 1991-2020 - normals-daily...\n",
      "Reformatting files: 1991-2020 - normals-monthly...\n",
      "Reformatting files: 2006-2020 - normals-hourly...\n",
      "Reformatting files: 2006-2020 - normals-daily...\n",
      "Reformatting files: 2006-2020 - normals-monthly...\n",
      "CPU times: user 457 ms, sys: 958 ms, total: 1.42 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# standardizing normal files\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        headerFile = f'{txtFiles_Folder}csv_headers/headers-{normal_period}-{normal_type}.txt'\n",
    "        normals_dict[normal_period][normal_type]['headerFile'] = headerFile\n",
    "        headers = []\n",
    "        with open(headerFile, 'r') as hfile:\n",
    "            lines = hfile.readlines()\n",
    "            for line in lines:\n",
    "                headers.append(line.strip('\\n'))\n",
    "\n",
    "        files_location = f'{normals_dict[normal_period][normal_type][\"files_location\"]}'    \n",
    "        if not os.path.exists(f'{files_location}formatted_files/'):\n",
    "                os.makedirs(f'{files_location}formatted_files/')\n",
    "\n",
    "        print(f'Reformatting files: {normal_period} - {normal_type}...')\n",
    "        normal_files = list(Path(f'{files_location}station_files/').glob('*.csv'))\n",
    "        normals_dict[normal_period][normal_type]['Total_Stations'] = len(normal_files)\n",
    "\n",
    "        part_format = partial(format_file, headers = headers, normal_type = normal_type) # make into partial function to be applied to map \n",
    "        pool = multiprocessing.Pool()\n",
    "        pool.map(part_format, normal_files)\n",
    "        pool.close()\n",
    "        \n",
    "        # print(f'{normal_period} - {normal_type} files formatted...\\n')"
   ]
  },
  {
   "source": [
    "Now to combine all the normal files together for the same type and same time period."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./data/station_normals/1981-2010/normals-hourly/combined_1981-2010-normals-hourly.csv exists...\n./data/station_normals/1981-2010/normals-daily/combined_1981-2010-normals-daily.csv exists...\n./data/station_normals/1981-2010/normals-monthly/combined_1981-2010-normals-monthly.csv exists...\n./data/station_normals/1991-2020/normals-hourly/combined_1991-2020-normals-hourly.csv exists...\n./data/station_normals/1991-2020/normals-daily/combined_1991-2020-normals-daily.csv exists...\n./data/station_normals/1991-2020/normals-monthly/combined_1991-2020-normals-monthly.csv exists...\n./data/station_normals/2006-2020/normals-hourly/combined_2006-2020-normals-hourly.csv exists...\n./data/station_normals/2006-2020/normals-daily/combined_2006-2020-normals-daily.csv exists...\n./data/station_normals/2006-2020/normals-monthly/combined_2006-2020-normals-monthly.csv exists...\nCPU times: user 5.38 ms, sys: 6 ms, total: 11.4 ms\nWall time: 32.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        files_location = f'{normals_dict[normal_period][normal_type][\"files_location\"]}'\n",
    "        files_toCombine = Path(f'{files_location}formatted_files').glob('*.csv')\n",
    "        combined_file = f'{files_location}combined_{normal_period}-{normal_type}.csv'\n",
    "        combine_Files(combined_file, files_toCombine)\n",
    "        normals_dict[normal_period][normal_type]['combined_file'] = combined_file\n"
   ]
  },
  {
   "source": [
    "Connect to postgres server to create a database for the climate normals: climate_normals_db. \n",
    "\n",
    "The tables will be:\n",
    "- states\n",
    "- HSI_1981_2010      : 1981-2010 hourly station inventory\n",
    "- HVAR_1981_2010     : 1981-2010 hourly variables : \n",
    "- HNORMALS_1981_2010 : 1981-2010 hourly normals\n",
    "- DSI_1981_2010      : 1981-2010 daily station inventory\n",
    "- DVAR_1981_2010     : 1981-2010 daily variables\n",
    "- DNORMALS_1981_2010 : 1981-2010 daily normals\n",
    "- MSI_1981_2010      : 1981-2010 monthly station inventory\n",
    "- MVAR_1981_2010     : 1981-2010 monthly variables\n",
    "- MNORMALS_1981_2010 : 1981-2010 monthly normals\n",
    "- HSI_1991_2020      : 1991-2020 hourly station inventory\n",
    "- HVAR_1991_2020     : 1991-2020 hourly variables\n",
    "- HNORMALS_1991_2020 : 1991-2020 hourly normals\n",
    "- DSI_1991_2020      : 1991-2020 daily station inventory\n",
    "- DVAR_1991_2020     : 1991-2020 daily variables\n",
    "- DNORMALS_1991_2020 : 1991-2020 daily normals\n",
    "- MSI_1991_2020      : 1991-2020 monthly station inventory\n",
    "- MVAR_1991_2020     : 1991-2020 monthly variables\n",
    "- MNORMALS_1991_2020 : 1991-2020 monthly normals\n",
    "- HSI_2006_2020      : 2006-2020 hourly station inventory\n",
    "- HVAR_2006_2020     : 2006-2020 hourly variables\n",
    "- HNORMALS_2006_2020 : 2006-2020 hourly normals\n",
    "- DSI_2006_2020      : 2006-2020 daily station inventory\n",
    "- DVAR_2006_2020     : 2006-2020 daily variables\n",
    "- DNORMALS_2006_2020 : 2006-2020 daily normals\n",
    "- MSI_2006_2020      : 2006-2020 monthly station inventory\n",
    "- MVAR_2006_2020     : 2006-2020 monthly variables\n",
    "- MNORMALS_2006_2020 : 2006-2020 monthly normals"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "def config(filename='./sql/database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    "\n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "    return db\n",
    "\n",
    "def connect_db(db_Name):\n",
    "    \"\"\" Connect to the database\"\"\"\n",
    "    try:\n",
    "        # read connection parameters\n",
    "        params = config()\n",
    "\n",
    "        # connect to the PostgreSQL server\n",
    "        print(f'Connecting to the {db_Name} database...')\n",
    "        conn = psql.connect(database=db_Name, **params)\n",
    "\n",
    "    except psql.OperationalError as e:\n",
    "        print(f'There is no database named {db_Name}...')\n",
    "        create_db(db_Name)\n",
    "        conn.close()\n",
    "        return connect_db(db_Name)\n",
    "    else:\n",
    "        print(f'***Connected: {db_Name}***')\n",
    "        return conn\n",
    "        \n",
    "def create_db(db_Name):\n",
    "    # read connection parameters\n",
    "    params = config()\n",
    "    print(f'Connecting to postgres database to create new database:{db_Name}')\n",
    "    conn = psql.connect(database='postgres',**params)\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    sql=f'CREATE database {db_Name}'\n",
    "\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        cursor.close()\n",
    "    except psql.errors.lookup('42P04'): #psql error code for duplicatedatabase\n",
    "        conn.close()\n",
    "        cursor.close()\n",
    "        return print(f\"Database: [{db_Name}] already exists....\")\n",
    "    else:\n",
    "        conn.close()\n",
    "        cursor.close()\n",
    "        return print(f\"***Database: {db_Name} created successfully***\")\n",
    "\n",
    "def execute_query(connection, query):\n",
    "    connection.autocommit = True\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        print(\"Query executed successfully\")\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "\n",
    "def execute_read_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connecting to the climate_normals_db database...\n***Connected: climate_normals_db***\n"
     ]
    }
   ],
   "source": [
    "conn = connect_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query executed successfully\n"
     ]
    }
   ],
   "source": [
    "create_state_table = \"\"\"\n",
    "DROP TABLE states;\n",
    "CREATE TABLE IF NOT EXISTS states(\n",
    "    statelong TEXT NOT NULL,\n",
    "    stateabbr CHAR(2) PRIMARY KEY\n",
    ")\n",
    "\"\"\"\n",
    "execute_query(conn, create_state_table)\n",
    "\n",
    "cur = conn.cursor()\n",
    "with open(f'{txtFiles_Folder}states.csv', 'r') as csv:\n",
    "    next(csv) # skips header\n",
    "    cur.copy_from(csv, 'states', sep=',')\n",
    "\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query executed successfully\nQuery executed successfully\nQuery executed successfully\nQuery executed successfully\nQuery executed successfully\nQuery executed successfully\nQuery executed successfully\nQuery executed successfully\nQuery executed successfully\n"
     ]
    }
   ],
   "source": [
    "# creation of variable tables\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        if normal_type == 'normals-hourly': temp = 'hly'\n",
    "        elif normal_type == 'normals-daily': temp = 'dly'\n",
    "        elif normal_type == 'normals-monthly': temp = 'mly'      \n",
    "        tableName = f'var_{temp}_{normal_period}'.replace('-','_')   # Replaces - with _ to play nicer with postgres. \n",
    "\n",
    "        create_variable_table = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {tableName};\n",
    "        CREATE TABLE IF NOT EXISTS {tableName}(\n",
    "            variable TEXT PRIMARY KEY,\n",
    "            description TEXT NOT NULL,\n",
    "            units TEXT NOT NULL\n",
    "        )\n",
    "        \"\"\"\n",
    "        execute_query(conn, create_variable_table)\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "        with open(normals_dict[normal_period][normal_type]['variables_file'], 'r') as csv:\n",
    "            cur.copy_from(csv, tableName, sep=',')\n",
    "        conn.commit()\n",
    "        cur.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n",
      "Query executed successfully\n"
     ]
    }
   ],
   "source": [
    "# creation of station inventory tables\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        if normal_type == 'normals-hourly': temp = 'hly'\n",
    "        elif normal_type == 'normals-daily': temp = 'dly'\n",
    "        elif normal_type == 'normals-monthly': temp = 'mly'        \n",
    "        tableName = f'si_{temp}_{normal_period}'.replace('-','_')\n",
    "        \n",
    "        # create_SI_tables = f\"\"\"\n",
    "        # DROP TABLE IF EXISTS {tableName};\"\"\"\n",
    "        # execute_query(conn, create_SI_tables)\n",
    "\n",
    "        create_SI_tables = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {tableName} CASCADE;\n",
    "        CREATE TABLE IF NOT EXISTS {tableName}\n",
    "        (\n",
    "            \"stationID\" TEXT PRIMARY KEY,\n",
    "            latitude  NUMERIC NOT NULL,\n",
    "            longitude NUMERIC NOT NULL,\n",
    "            elevation NUMERIC NOT NULL,\n",
    "            state CHAR(2) NOT NULL,\n",
    "            \"stationName\" TEXT NOT NULL,\n",
    "            network TEXT,\n",
    "            \"wmoID\" TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # execute_query(conn, create_SI_tables)\n",
    "        # cur = conn.cursor()\n",
    "        # with open(normals_dict[normal_period][normal_type]['inventory_file'], 'r') as csv:\n",
    "        #     next(csv) #skips header\n",
    "        #     cur.copy_from(csv, tableName, sep=',')\n",
    "\n",
    "        # conn.commit()\n",
    "\n",
    "        # alter_SI_tables = f\"\"\"\n",
    "        # ALTER TABLE {tableName} ADD COLUMN coordinates GEOMETRY(POINT, 4326);\n",
    "        # UPDATE {tableName} SET coordinates = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326);\n",
    "        # \"\"\"\n",
    "        # execute_query(conn, alter_SI_tables)\n",
    "        # cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query executed successfully\n",
      "normals_hly_1981_2010 created and filled...\n",
      "Query executed successfully\n",
      "normals_dly_1981_2010 created and filled...\n",
      "Query executed successfully\n",
      "normals_mly_1981_2010 created and filled...\n",
      "Query executed successfully\n",
      "normals_hly_1991_2020 created and filled...\n",
      "Query executed successfully\n",
      "normals_dly_1991_2020 created and filled...\n",
      "Query executed successfully\n",
      "normals_mly_1991_2020 created and filled...\n",
      "Query executed successfully\n",
      "normals_hly_2006_2020 created and filled...\n",
      "Query executed successfully\n",
      "normals_dly_2006_2020 created and filled...\n",
      "Query executed successfully\n",
      "normals_mly_2006_2020 created and filled...\n"
     ]
    }
   ],
   "source": [
    "# creation of normals tables\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        if normal_type == 'normals-hourly': temp = 'hly'\n",
    "        elif normal_type == 'normals-daily': temp = 'dly'\n",
    "        elif normal_type == 'normals-monthly': temp = 'mly'        \n",
    "        tableName = f'normals_{temp}_{normal_period}'.replace('-','_')\n",
    "\n",
    "        normals_list = '\"stationID\" TEXT NOT NULL, date TEXT NOT NULL, month SMALLINT NOT NULL, day SMALLINT NOT NULL, hour SMALLINT NOT NULL,'\n",
    "\n",
    "        with open(normals_dict[normal_period][normal_type]['headerFile'], 'r') as header:\n",
    "            lines = header.readlines()[5:]\n",
    "\n",
    "        for count, line in enumerate(lines):\n",
    "            var = line.strip('\\n').split(',')[0]\n",
    "            if '1STDIR' in var or '2NDDIR' in var:\n",
    "                normals_list += f'\"{var}\" TEXT,'\n",
    "                continue\n",
    "\n",
    "            if normal_period == '1981-2010':\n",
    "                if count % 2 == 0:\n",
    "                    normals_list += f'\"{var}\" NUMERIC,'\n",
    "                else:\n",
    "                    normals_list += f'\"{var}\" TEXT,'\n",
    "                continue\n",
    "\n",
    "            if count % 4 == 0 or count % 4 == 3:\n",
    "                normals_list += f'\"{var}\" NUMERIC,'\n",
    "            else:\n",
    "                normals_list += f'\"{var}\" TEXT,'\n",
    "                \n",
    "        normals_list = normals_list.rstrip(',')\n",
    "            \n",
    "        parentTable = f'si_{temp}_{normal_period}'.replace('-','_')\n",
    "        create_normals_tables = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {tableName};\n",
    "        CREATE TABLE IF NOT EXISTS {tableName}\n",
    "        (\n",
    "            {normals_list},\n",
    "            CONSTRAINT \"fk_stationID\"\n",
    "                FOREIGN KEY(\"stationID\")\n",
    "                    REFERENCES {parentTable}(\"stationID\")\n",
    "                    ON DELETE CASCADE\n",
    "        )\n",
    "        \"\"\"\n",
    "        # execute_query(conn, create_normals_tables)\n",
    "\n",
    "        # cur = conn.cursor()\n",
    "        # with open(normals_dict[normal_period][normal_type]['combined_file'], 'r') as csv:\n",
    "        #     next(csv) #skips header\n",
    "        #     cur.copy_from(csv, tableName, sep=',', null=\"\")\n",
    "        # print(f'{tableName} created and filled...')\n",
    "        # conn.commit()\n",
    "        # cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'files_location': './data/station_normals/2006-2020/normals-monthly/',\n",
       " 'inventory_file': './txt_files/station_inventory/station-inventory-2006-2020_normals-monthly.csv',\n",
       " 'variables_file': './txt_files/variables/variables-2006-2020-normals-monthly.csv',\n",
       " 'headerFile': './txt_files/csv_headers/headers-2006-2020-normals-monthly.txt',\n",
       " 'Total_Stations': 13471,\n",
       " 'combined_file': './data/station_normals/2006-2020/normals-monthly/combined_2006-2020-normals-monthly.csv'}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "normals_dict[normal_period][normal_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "8829f2f28fb4fe9df99e8c305ffe937ba372ff45bcdf063cdbd318f5788220f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}