{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa25044e-be81-46e7-9a69-1073cf57b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "import psycopg2 as psql\n",
    "import creds\n",
    "\n",
    "dataFolder = './data/station_normals/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bf430-c364-4444-b076-8b9dec0dba11",
   "metadata": {},
   "source": [
    "* 1981-2010\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1981-2010/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1981-2010/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1981-2010/archive/)\n",
    "    \n",
    "* 1991-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1991-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1991-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1991-2020/archive/)\n",
    "    \n",
    "* 2006-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/2006-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/2006-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/2006-2020/archive/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_Files(combined_file, files_toCombine):\n",
    "    if os.path.exists(combined_file):\n",
    "        return print(f'{combined_file} exists...')\n",
    "    \n",
    "    with open(combined_file, 'w') as outfile:\n",
    "        initial = 1\n",
    "        for file in files_toCombine:\n",
    "            with open(file, 'r') as infile:\n",
    "                header = infile.readline()\n",
    "                if initial:\n",
    "                    outfile.write(f'{header}')\n",
    "                    initial = 0\n",
    "                outfile.write(infile.read())\n",
    "    return print(f'{combined_file} created...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20dddc34-1e3e-44f6-a355-46d6f3b846c0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1981-2010 normals-hourly already downloaded...\n1981-2010 normals-daily already downloaded...\n1981-2010 normals-monthly already downloaded...\n1991-2020 normals-hourly already downloaded...\n1991-2020 normals-daily already downloaded...\n1991-2020 normals-monthly already downloaded...\n2006-2020 normals-hourly already downloaded...\n2006-2020 normals-daily already downloaded...\n2006-2020 normals-monthly already downloaded...\n"
     ]
    }
   ],
   "source": [
    "import climate_normal_scripts as cns\n",
    "importlib.reload(cns) # reloads script if script was changed after notebook kernel started\n",
    "\n",
    "# get climate normals files\n",
    "normal_periods = ['1981-2010','1991-2020','2006-2020']\n",
    "normal_types = ['normals-hourly','normals-daily', 'normals-monthly']\n",
    "main_url = 'https://www.ncei.noaa.gov/data/'\n",
    "\n",
    "climates = {}\n",
    "for normal_period in normal_periods:\n",
    "    climates[normal_period] = {}\n",
    "    for normal_type in normal_types:\n",
    "        location = f'{dataFolder}{normal_period}/{normal_type}/'\n",
    "        cns.get_climate_normals(main_url, normal_type, normal_period, location)\n",
    "        climates[normal_period][normal_type] = {'location': location, 'inventory_file': f'{location}station-inventory.csv'}\n",
    "        \n",
    "        # file_list.append(f'{location}station-inventory.csv')\n",
    "    \n",
    "# mainInventory_file = f'./txt_files/station_inventory/all-stations.csv'  \n",
    "# combine_Files(mainInventory_file, file_list)\n",
    "# del file_list\n",
    "# # opens massive csv file into pandas DataFrame to drop duplicate rows and then resaves csv file\n",
    "# # note some station id values may be repeated due to other fields being different\n",
    "# df = pd.read_csv(mainInventory_file)\n",
    "# df = df.drop_duplicates(keep='last').sort_values(by=['state', 'name']).reset_index(drop=True)\n",
    "# df.wmo_id = df.wmo_id.astype(dtype='Int64')\n",
    "# df.to_csv(mainInventory_file,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(df, normal_type):\n",
    "    '''\n",
    "    Function which reads in the DATE field and compares to the expected Full time range. If there are any missing time records, those missing times are added\n",
    "    into the record, with nan values for the corresponding weather climate normals for those missing times. Populates the month,day,hour fields based off the new DATE field. \n",
    "\n",
    "    Years are dummy values. For Daily files, the year is set to a leap year to also include the Leap day. Leap day is needed in hours and months files only.\n",
    "    '''\n",
    "\n",
    "    if normal_type == 'normals-hourly':\n",
    "        full_range = pd.date_range(start= '1900-01-01 00:00:00', end = '1900-12-31 23:00:00', freq = 'H') # timeindex for all hours \n",
    "        dateOUT_format = '%b-%d %H:%M'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m-%dT%X')                                            # reformats DATE column into timeindex for comparison\n",
    "        \n",
    "    elif normal_type == 'normals-daily':\n",
    "        full_range = pd.date_range(start= '1904-01-01', end = '1904-12-31', freq = 'D')\n",
    "        dateOUT_format = '%b-%d'\n",
    "        df.DATE = pd.to_datetime(df.DATE + '-1904', format = '%m-%d-%Y')                                  # reformats DATE column, adds year column to avoid error for leap year\n",
    "    \n",
    "    elif normal_type == 'normals-monthly':\n",
    "        full_range = pd.date_range(start= '1900-01', end = '1900-12', freq = 'MS')\n",
    "        dateOUT_format = '%b'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m')                                                  # reformats DATE column\n",
    "\n",
    "    df = df.set_index('DATE').reindex(full_range)                                                     # sets DATE column to index then adds rows to record based off missing dates\n",
    "    df.reset_index(inplace=True,drop=True)                                                            # resets index back to default dropping old DATE column\n",
    "    df['DATE'] = full_range.strftime(dateOUT_format)\n",
    "    \n",
    "    df.month = full_range.month\n",
    "    df.day = full_range.day\n",
    "    df.hour = full_range.hour\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_hourly_variables(df, normal_period):\n",
    "    headers= df.columns\n",
    "    ignore_headers = ['STATION', 'DATE','month', 'day', 'hour','HLY-WIND-VCTDIR', 'HLY-WIND-1STDIR', 'HLY-WIND-2NDDIR']    # fields to not be changed to floats\n",
    "    \n",
    "    wind_dir = ['HLY-WIND-1STDIR', 'HLY-WIND-2NDDIR']                                                                      # fields that will be changed to wind direction label based off value from 1-8\n",
    "    wind_dir_labels = {1.0:'N', 2.0:'NE', 3.0:'E', 4.0:'SE', 5.0: 'S', 6.0:'SW', 7.0:'W', 8.0:'NW'} \n",
    "    df[wind_dir] = df[wind_dir].replace(wind_dir_labels)\n",
    "\n",
    "    if normal_period == '1981-2010':\n",
    "        ignore_headers += ['_ATTRIBUTES']  \n",
    "    else:\n",
    "        ignore_headers += ['_flags', 'years_']  \n",
    "\n",
    "    normal_variables = [x for x in headers if not any(y in x for y in ignore_headers)]                # grabs all headers to be formatted   \n",
    "    if normal_period == '1981-2010':\n",
    "        df[normal_variables] = df[normal_variables].divide(10)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_daily_variables(df, normal_period):\n",
    "    headers= df.columns\n",
    "    ignore_headers = ['STATION', 'DATE','month', 'day', 'hour', 'CLDD', 'HTDD', 'GRDD']    # fields to not be formatted/changed\n",
    "\n",
    "    if normal_period == '1981-2010':\n",
    "        ignore_headers += ['_ATTRIBUTES']  \n",
    "    else:\n",
    "        ignore_headers += ['_flags', 'years_']  \n",
    "\n",
    "    normal_variables = [x for x in headers if not any(y in x for y in ignore_headers)]                # grabs all headers to be formatted   \n",
    "    \n",
    "    if normal_period == '1981-2010':\n",
    "        # convert precipitation values to hundredths of inches\n",
    "        prcp_variables  = [ x for x in normal_variables if 'PRCP' in x and 'PCTALL' not in x]\n",
    "        df[prcp_variables] = df[prcp_variables].divide(100)\n",
    "\n",
    "        snwdpctl_variables  = [ x for x in normal_variables if 'SNWD' in x and 'PCTALL' not in x]    # snow depth variables are whole numbers\n",
    "\n",
    "        # convert to tenths float\n",
    "        normal_variables = [x for x in normal_variables if x not in prcp_variables and x not in snwdpctl_variables]  # removing variables already formatted\n",
    "        df[normal_variables] = df[normal_variables].divide(10)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_monthly_variables(df, normal_period):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  STATION      DATE     month  day  hour  DLY-TAVG-NORMAL  \\\n",
       "0  AQC00914000  Jan-01  1      1    0    NaN                \n",
       "1  AQC00914000  Jan-02  1      2    0    NaN                \n",
       "\n",
       "   DLY-TAVG-NORMAL_ATTRIBUTES  DLY-TAVG-STDDEV  DLY-TAVG-STDDEV_ATTRIBUTES  \\\n",
       "0 NaN                         NaN              NaN                           \n",
       "1 NaN                         NaN              NaN                           \n",
       "\n",
       "   DLY-TMAX-NORMAL  ...  DLY-SNWD-75PCTL  DLY-SNWD-75PCTL_ATTRIBUTES  \\\n",
       "0 NaN               ... NaN              NaN                           \n",
       "1 NaN               ... NaN              NaN                           \n",
       "\n",
       "   DLY-SNWD-PCTALL-GE001WI  DLY-SNWD-PCTALL-GE001WI_ATTRIBUTES  \\\n",
       "0  0.0                      P                                    \n",
       "1  0.0                      P                                    \n",
       "\n",
       "   DLY-SNWD-PCTALL-GE003WI  DLY-SNWD-PCTALL-GE003WI_ATTRIBUTES  \\\n",
       "0  0.0                      P                                    \n",
       "1  0.0                      P                                    \n",
       "\n",
       "   DLY-SNWD-PCTALL-GE005WI  DLY-SNWD-PCTALL-GE005WI_ATTRIBUTES  \\\n",
       "0  0.0                      P                                    \n",
       "1  0.0                      P                                    \n",
       "\n",
       "   DLY-SNWD-PCTALL-GE010WI  DLY-SNWD-PCTALL-GE010WI_ATTRIBUTES  \n",
       "0  0.0                      P                                   \n",
       "1  0.0                      P                                   \n",
       "\n",
       "[2 rows x 125 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th></th>\n      <th>STATION</th>\n      <th>DATE</th>\n      <th>month</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>DLY-TAVG-NORMAL</th>\n      <th>DLY-TAVG-NORMAL_ATTRIBUTES</th>\n      <th>DLY-TAVG-STDDEV</th>\n      <th>DLY-TAVG-STDDEV_ATTRIBUTES</th>\n      <th>DLY-TMAX-NORMAL</th>\n      <th>...</th>\n      <th>DLY-SNWD-75PCTL</th>\n      <th>DLY-SNWD-75PCTL_ATTRIBUTES</th>\n      <th>DLY-SNWD-PCTALL-GE001WI</th>\n      <th>DLY-SNWD-PCTALL-GE001WI_ATTRIBUTES</th>\n      <th>DLY-SNWD-PCTALL-GE003WI</th>\n      <th>DLY-SNWD-PCTALL-GE003WI_ATTRIBUTES</th>\n      <th>DLY-SNWD-PCTALL-GE005WI</th>\n      <th>DLY-SNWD-PCTALL-GE005WI_ATTRIBUTES</th>\n      <th>DLY-SNWD-PCTALL-GE010WI</th>\n      <th>DLY-SNWD-PCTALL-GE010WI_ATTRIBUTES</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AQC00914000</td>\n      <td>Jan-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>P</td>\n      <td>0.0</td>\n      <td>P</td>\n      <td>0.0</td>\n      <td>P</td>\n      <td>0.0</td>\n      <td>P</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AQC00914000</td>\n      <td>Jan-02</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>P</td>\n      <td>0.0</td>\n      <td>P</td>\n      <td>0.0</td>\n      <td>P</td>\n      <td>0.0</td>\n      <td>P</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 125 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "\n",
    "# standardizing normal files  \n",
    "normal_period = '1981-2010'\n",
    "# normal_period = '1991-2020'\n",
    "normal_type = 'normals-hourly'\n",
    "normal_type = 'normals-daily'\n",
    "# normal_type = 'normals-monthly'\n",
    "\n",
    "\n",
    "# for normal_period in normal_periods:\n",
    "#     for normal_type in normal_types:\n",
    "\n",
    "headerFile = f'./txt_files/csv_headers/headers-{normal_period}-{normal_type}.txt'\n",
    "reformatted_File = f'{climates[normal_period][normal_type][\"location\"]}test.csv'  \n",
    "headers = []\n",
    "with open(headerFile, 'r') as hfile:\n",
    "    lines = hfile.readlines()\n",
    "    for line in lines:\n",
    "        headers.append(line.strip('\\n'))\n",
    "     \n",
    "normal_files = Path(f'{climates[normal_period][normal_type][\"location\"]}station_files/').glob('*.csv')\n",
    "for normal_file in normal_files:\n",
    "    df = pd.read_csv(normal_file)\n",
    "    df = df.reindex(columns=headers) # standardize headers: adds missing columns(if any) and reorders columns based off headerFile\n",
    "    df = df.replace(to_replace=[-9999.0, -7777.0, -6666.0, -4444.0, ' '], value= np.nan) \n",
    "    df = format_date(df, normal_period, normal_type)\n",
    "\n",
    "    ## now to do formatting for individual variables based off documentation\n",
    "    if normal_type == 'normals-hourly':\n",
    "        df = format_hourly_variables(df, normal_period)\n",
    "    elif normal_type == 'normals-daily':\n",
    "        df = format_daily_variables(df, normal_period)\n",
    "        pass\n",
    "    elif normal_type == 'normals-monthly':\n",
    "        # df = format_monthly_variables(df, normal_period)\n",
    "        pass\n",
    "\n",
    "\n",
    "    station_meta = {'STATION': df.STATION[0]} # station metadata to replace NaN in the metadata column\n",
    "    df.fillna(value= station_meta, inplace=True)\n",
    "    df = df.reindex(columns=headers) # ensures columns in correct order after formatting file\n",
    "    break\n",
    "\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_periods = ['1981-2010','1991-2020','2006-2020']\n",
    "normal_types = ['normals-hourly','normals-daily', 'normals-monthly']\n",
    "\n",
    "for normal_period in normal_periods:\n",
    "    header_list =[]\n",
    "    for normal_type in normal_types:\n",
    "        combined_file = f'{climates[normal_period][normal_type][\"location\"]}{normal_type}-{normal_period}.csv'\n",
    "        files_location = f'{climates[normal_period][normal_type][\"location\"]}station_files/'\n",
    "        files_toCombine = Path(files_location).glob('*.csv')\n",
    "        # combine_Files(combined_file, files_toCombine)\n",
    "\n",
    "        for file in files_toCombine:\n",
    "            with open(file, 'r') as infile:\n",
    "                header_list.append([file.name, infile.readline()])\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(db_Name):\n",
    "    try:\n",
    "        conn = psql.connect(\n",
    "                user=creds.USER,\n",
    "                password=creds.PASS,\n",
    "                host=creds.HOST,\n",
    "                port=creds.PORT,\n",
    "                database=db_Name\n",
    "                )\n",
    "    except psql.OperationalError as e:\n",
    "        print(f'There is no database named {db_Name}...')\n",
    "        create_db(db_Name)\n",
    "        return connect_db(db_Name)\n",
    "    else:\n",
    "        print(f'Connected: {db_Name}')\n",
    "        return conn\n",
    "        \n",
    "def create_db(db_Name):\n",
    "    conn = psql.connect(\n",
    "                user=creds.USER,\n",
    "                password=creds.PASS,\n",
    "                host=creds.HOST,\n",
    "                port=creds.PORT,\n",
    "                database='postgres'\n",
    "                )\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    sql=f'CREATE database {db_Name}'\n",
    "\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except psql.errors.lookup('42P04'): #psql error code for duplicatedatabase\n",
    "        conn.close()\n",
    "        return print(f\"....Database: {db_Name} already exists....\")\n",
    "    else:\n",
    "        print(f'....Database: {db_Name} created....')      \n",
    "        conn.close()\n",
    "        return print(f\"....Database: {db_Name} created successfully....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connected: climate_normals_db\n"
     ]
    }
   ],
   "source": [
    "a = connect_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "....Database: climate_normals_db already exists....\n"
     ]
    }
   ],
   "source": [
    "create_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "baselines = [['STATION'], ['DATE'],['month'],['day'],['hour']]\n",
    "for normal_period in normal_periods:\n",
    "    for normal_type in normal_types:\n",
    "        headerFile = f'./txt_files/csv_headers/headers-{normal_period}-{normal_type}.txt'\n",
    "        varFile = f'./txt_files/variables/variables-{normal_period}-{normal_type}.csv'\n",
    "\n",
    "        with open(headerFile, 'w' , newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerows(baselines)\n",
    "            with open(varFile, 'r') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                for row in reader:\n",
    "                    writer.writerow([row[0]])\n",
    "                    if normal_period == '1981-2010':\n",
    "                        writer.writerow([row[0] + '_ATTRIBUTES'])\n",
    "                    else:\n",
    "                        writer.writerow(['meas_flag_' + row[0]])\n",
    "                        writer.writerow(['comp_flag_' + row[0]])\n",
    "                        writer.writerow(['years_' + row[0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd08829f2f28fb4fe9df99e8c305ffe937ba372ff45bcdf063cdbd318f5788220f1",
   "display_name": "Python 3.9.2 64-bit ('Climate-Normals': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}