{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa25044e-be81-46e7-9a69-1073cf57b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "import psycopg2 as psql\n",
    "import creds\n",
    "\n",
    "dataFolder = './data/station_normals/'\n",
    "txtFiles_Folder = './txt_files/'\n",
    "normals_dict = { '1981-2010': {'normals-hourly': { },'normals-daily': { }, 'normals-monthly': { }},\n",
    "                 '1991-2020': {'normals-hourly': { },'normals-daily': { }, 'normals-monthly': { }},\n",
    "                 '2006-2020': {'normals-hourly': { },'normals-daily': { }, 'normals-monthly': { }}\n",
    "                 }   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bf430-c364-4444-b076-8b9dec0dba11",
   "metadata": {},
   "source": [
    "* 1981-2010\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1981-2010/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1981-2010/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1981-2010/archive/)\n",
    "    \n",
    "* 1991-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1991-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1991-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1991-2020/archive/)\n",
    "    \n",
    "* 2006-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/2006-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/2006-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/2006-2020/archive/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_Files(combined_file, files_toCombine):\n",
    "    if os.path.exists(combined_file):\n",
    "        return print(f'{combined_file} exists...')\n",
    "    \n",
    "    with open(combined_file, 'w') as outfile:\n",
    "        initial = 1\n",
    "        for file in files_toCombine:\n",
    "            with open(file, 'r') as infile:\n",
    "                header = infile.readline()\n",
    "                if initial:\n",
    "                    outfile.write(f'{header}')\n",
    "                    initial = 0\n",
    "                outfile.write(infile.read())\n",
    "    return print(f'{combined_file} created...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "20dddc34-1e3e-44f6-a355-46d6f3b846c0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1981-2010 normals-hourly already downloaded...\n1981-2010 normals-daily already downloaded...\n1981-2010 normals-monthly already downloaded...\n1991-2020 normals-hourly already downloaded...\n1991-2020 normals-daily already downloaded...\n1991-2020 normals-monthly already downloaded...\n2006-2020 normals-hourly already downloaded...\n2006-2020 normals-daily already downloaded...\n2006-2020 normals-monthly already downloaded...\nHeaders files generated at ./txt_files/csv_headers/\n"
     ]
    }
   ],
   "source": [
    "import climate_normal_scripts as cns\n",
    "importlib.reload(cns) # reloads script if script was changed after notebook kernel started\n",
    "\n",
    "# get climate normals files\n",
    "normal_periods = ['1981-2010','1991-2020','2006-2020']\n",
    "normal_types = ['normals-hourly','normals-daily', 'normals-monthly']\n",
    "main_url = 'https://www.ncei.noaa.gov/data/'\n",
    "\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    for normal_type in normal_types:\n",
    "        location = f'{dataFolder}{normal_period}/{normal_type}/'\n",
    "        cns.get_climate_normals(main_url, normal_type, normal_period, location)\n",
    "        normals_dict[normal_period][normal_type] = {'files_location': location,\n",
    "                                                   'inventory_file': f'{txtFiles_Folder}station-inventory-{normal_period}_{normal_type}.csv'}\n",
    "\n",
    "cns.generate_header_files()\n",
    "    \n",
    "# mainInventory_file = f'{txtFiles_Folder}station_inventory/all-stations.csv'  \n",
    "# combine_Files(mainInventory_file, file_list)\n",
    "# del file_list\n",
    "# # opens massive csv file into pandas DataFrame to drop duplicate rows and then resaves csv file\n",
    "# # note some station id values may be repeated due to other fields being different\n",
    "# df = pd.read_csv(mainInventory_file)\n",
    "# df = df.drop_duplicates(keep='last').sort_values(by=['state', 'name']).reset_index(drop=True)\n",
    "# df.wmo_id = df.wmo_id.astype(dtype='Int64')\n",
    "# df.to_csv(mainInventory_file,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(df, normal_type):\n",
    "    '''\n",
    "    Function which reads in the DATE field and compares to the expected Full time range. If there are any missing time records, those missing times are added\n",
    "    into the record, with nan values for the corresponding weather climate normals for those missing times. Populates the month,day,hour fields based off the new DATE field. \n",
    "\n",
    "    Years are dummy values. For Daily files, the year is set to a leap year to also include the Leap day. Leap day is needed in hours and months files only.\n",
    "    '''\n",
    "\n",
    "    if normal_type == 'normals-hourly':\n",
    "        full_range = pd.date_range(start= '1900-01-01 00:00:00', end = '1900-12-31 23:00:00', freq = 'H') # timeindex for all hours \n",
    "        dateOUT_format = '%b-%d %H:%M'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m-%dT%X')                                            # reformats DATE column into timeindex for comparison\n",
    "        \n",
    "    elif normal_type == 'normals-daily':\n",
    "        full_range = pd.date_range(start= '1904-01-01', end = '1904-12-31', freq = 'D')\n",
    "        dateOUT_format = '%b-%d'\n",
    "        df.DATE = pd.to_datetime(df.DATE + '-1904', format = '%m-%d-%Y')                                  # reformats DATE column, adds year column to avoid error for leap year\n",
    "    \n",
    "    elif normal_type == 'normals-monthly':\n",
    "        full_range = pd.date_range(start= '1900-01', end = '1900-12', freq = 'MS')\n",
    "        dateOUT_format = '%b'\n",
    "        df.DATE = pd.to_datetime(df.DATE, format = '%m')                                                  # reformats DATE column\n",
    "\n",
    "    df = df.set_index('DATE').reindex(full_range)                                                     # sets DATE column to index then adds rows to record based off missing dates\n",
    "    df.reset_index(inplace=True,drop=True)                                                            # resets index back to default dropping old DATE column\n",
    "    df['DATE'] = full_range.strftime(dateOUT_format)\n",
    "    \n",
    "    df.month = full_range.month\n",
    "    df.day = full_range.day\n",
    "    df.hour = full_range.hour\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_variables(df, headersFormat):\n",
    "    tenths = [header.split(',')[0] for header in headersFormat if header.split(',')[1] == 'Tenths']\n",
    "    hundredths = [header.split(',')[0] for header in headersFormat if header.split(',')[1] == 'Hundredths']\n",
    "    wind_dir = [header.split(',')[0] for header in headersFormat if header.split(',')[1] == 'Wind_Direction']\n",
    "    wind_dir_labels = {1.0:'N', 2.0:'NE', 3.0:'E', 4.0:'SE', 5.0: 'S', 6.0:'SW', 7.0:'W', 8.0:'NW'} \n",
    "\n",
    "    df[tenths] = df[tenths].divide(10)\n",
    "    df[hundredths] = df[hundredths].divide(100)\n",
    "    df[wind_dir] = df[wind_dir].replace(wind_dir_labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  STATION      DATE           month  day  hour  HLY-TEMP-NORMAL  \\\n",
       "0  AQW00061705  Jan-01 00:00  1      1    0     80.4              \n",
       "1  AQW00061705  Jan-01 01:00  1      1    1     80.3              \n",
       "\n",
       "  HLY-TEMP-NORMAL_ATTRIBUTES  HLY-TEMP-10PCTL HLY-TEMP-10PCTL_ATTRIBUTES  \\\n",
       "0  P                          75.9             P                           \n",
       "1  C                          76.6             C                           \n",
       "\n",
       "   HLY-TEMP-90PCTL  ... HLY-CLOD-PCTCLR  HLY-CLOD-PCTCLR_ATTRIBUTES  \\\n",
       "0  83.8             ...  0.5             C                            \n",
       "1  83.5             ...  0.0             C                            \n",
       "\n",
       "  HLY-CLOD-PCTFEW  HLY-CLOD-PCTFEW_ATTRIBUTES HLY-CLOD-PCTSCT  \\\n",
       "0  7.3             C                           15.2             \n",
       "1  6.5             C                           14.0             \n",
       "\n",
       "   HLY-CLOD-PCTSCT_ATTRIBUTES HLY-CLOD-PCTBKN  HLY-CLOD-PCTBKN_ATTRIBUTES  \\\n",
       "0  C                           38.3            C                            \n",
       "1  C                           36.3            C                            \n",
       "\n",
       "  HLY-CLOD-PCTOVC  HLY-CLOD-PCTOVC_ATTRIBUTES  \n",
       "0  38.8            C                           \n",
       "1  43.2            C                           \n",
       "\n",
       "[2 rows x 57 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th></th>\n      <th>STATION</th>\n      <th>DATE</th>\n      <th>month</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>HLY-TEMP-NORMAL</th>\n      <th>HLY-TEMP-NORMAL_ATTRIBUTES</th>\n      <th>HLY-TEMP-10PCTL</th>\n      <th>HLY-TEMP-10PCTL_ATTRIBUTES</th>\n      <th>HLY-TEMP-90PCTL</th>\n      <th>...</th>\n      <th>HLY-CLOD-PCTCLR</th>\n      <th>HLY-CLOD-PCTCLR_ATTRIBUTES</th>\n      <th>HLY-CLOD-PCTFEW</th>\n      <th>HLY-CLOD-PCTFEW_ATTRIBUTES</th>\n      <th>HLY-CLOD-PCTSCT</th>\n      <th>HLY-CLOD-PCTSCT_ATTRIBUTES</th>\n      <th>HLY-CLOD-PCTBKN</th>\n      <th>HLY-CLOD-PCTBKN_ATTRIBUTES</th>\n      <th>HLY-CLOD-PCTOVC</th>\n      <th>HLY-CLOD-PCTOVC_ATTRIBUTES</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AQW00061705</td>\n      <td>Jan-01 00:00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>80.4</td>\n      <td>P</td>\n      <td>75.9</td>\n      <td>P</td>\n      <td>83.8</td>\n      <td>...</td>\n      <td>0.5</td>\n      <td>C</td>\n      <td>7.3</td>\n      <td>C</td>\n      <td>15.2</td>\n      <td>C</td>\n      <td>38.3</td>\n      <td>C</td>\n      <td>38.8</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AQW00061705</td>\n      <td>Jan-01 01:00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>80.3</td>\n      <td>C</td>\n      <td>76.6</td>\n      <td>C</td>\n      <td>83.5</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>C</td>\n      <td>6.5</td>\n      <td>C</td>\n      <td>14.0</td>\n      <td>C</td>\n      <td>36.3</td>\n      <td>C</td>\n      <td>43.2</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 57 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "\n",
    "# standardizing normal files  \n",
    "normal_period = '1981-2010'\n",
    "# normal_period = '1991-2020'\n",
    "normal_type = 'normals-hourly'\n",
    "# normal_type = 'normals-daily'\n",
    "# normal_type = 'normals-monthly'\n",
    "\n",
    "\n",
    "# for normal_period in normal_periods:\n",
    "#     for normal_type in normal_types:\n",
    "\n",
    "headerFile = f'{txtFiles_Folder}csv_headers/headers-{normal_period}-{normal_type}.txt'\n",
    "reformatted_File = f'{normals_dict[normal_period][normal_type][\"files_location\"]}test.csv'  \n",
    "headers = []\n",
    "headersFormat = []\n",
    "with open(headerFile, 'r') as hfile:\n",
    "    lines = hfile.readlines()\n",
    "    for line in lines:\n",
    "        headers.append(line.strip('\\n').split(',')[0])\n",
    "        headersFormat.append(line.strip('\\n'))\n",
    "     \n",
    "normal_files = Path(f'{normals_dict[normal_period][normal_type][\"files_location\"]}station_files/').glob('*.csv')\n",
    "for normal_file in normal_files:\n",
    "    df = pd.read_csv(normal_file)\n",
    "    df = df.reindex(columns=headers) # standardize headers: adds missing columns(if any) and reorders columns based off headerFile\n",
    "    df = df.replace(to_replace=[-9999.0, -7777.0, -6666.0, -4444.0, ' '], value= np.nan) \n",
    "    df = format_date(df, normal_type)\n",
    "    \n",
    "    df = format_variables(df, headersFormat[5:])\n",
    "    \n",
    "    station_meta = {'STATION': df.STATION[0]} # station metadata to replace NaN in the metadata column\n",
    "    df.fillna(value= station_meta, inplace=True)\n",
    "    df = df.reindex(columns=headers) # ensures columns in correct order after formatting file\n",
    "    break\n",
    "\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_periods = ['1981-2010','1991-2020','2006-2020']\n",
    "normal_types = ['normals-hourly','normals-daily', 'normals-monthly']\n",
    "\n",
    "for normal_period, normal_types in normals_dict.items():\n",
    "    header_list =[]\n",
    "    for normal_type in normal_types:\n",
    "        combined_file = f'{normals_dict[normal_period][normal_type][\"files_location\"]}{normal_type}-{normal_period}.csv'\n",
    "        files_location = f'{normals_dict[normal_period][normal_type][\"files_location\"]}station_files/'\n",
    "        files_toCombine = Path(files_location).glob('*.csv')\n",
    "        # combine_Files(combined_file, files_toCombine)\n",
    "\n",
    "        for file in files_toCombine:\n",
    "            with open(file, 'r') as infile:\n",
    "                header_list.append([file.name, infile.readline()])\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(db_Name):\n",
    "    try:\n",
    "        conn = psql.connect(\n",
    "                user=creds.USER,\n",
    "                password=creds.PASS,\n",
    "                host=creds.HOST,\n",
    "                port=creds.PORT,\n",
    "                database=db_Name\n",
    "                )\n",
    "    except psql.OperationalError as e:\n",
    "        print(f'There is no database named {db_Name}...')\n",
    "        create_db(db_Name)\n",
    "        return connect_db(db_Name)\n",
    "    else:\n",
    "        print(f'Connected: {db_Name}')\n",
    "        return conn\n",
    "        \n",
    "def create_db(db_Name):\n",
    "    conn = psql.connect(\n",
    "                user=creds.USER,\n",
    "                password=creds.PASS,\n",
    "                host=creds.HOST,\n",
    "                port=creds.PORT,\n",
    "                database='postgres'\n",
    "                )\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    sql=f'CREATE database {db_Name}'\n",
    "\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except psql.errors.lookup('42P04'): #psql error code for duplicatedatabase\n",
    "        conn.close()\n",
    "        return print(f\"....Database: {db_Name} already exists....\")\n",
    "    else:\n",
    "        print(f'....Database: {db_Name} created....')      \n",
    "        conn.close()\n",
    "        return print(f\"....Database: {db_Name} created successfully....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connected: climate_normals_db\n"
     ]
    }
   ],
   "source": [
    "a = connect_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "....Database: climate_normals_db already exists....\n"
     ]
    }
   ],
   "source": [
    "create_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd08829f2f28fb4fe9df99e8c305ffe937ba372ff45bcdf063cdbd318f5788220f1",
   "display_name": "Python 3.9.2 64-bit ('Climate-Normals': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}