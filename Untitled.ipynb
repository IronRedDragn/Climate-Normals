{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa25044e-be81-46e7-9a69-1073cf57b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "import psycopg2 as psql\n",
    "import creds\n",
    "\n",
    "dataFolder = './data/normals/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bf430-c364-4444-b076-8b9dec0dba11",
   "metadata": {},
   "source": [
    "* 1981-2010\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1981-2010/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1981-2010/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1981-2010/archive/)\n",
    "    \n",
    "* 1991-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/1991-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/1991-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/1991-2020/archive/)\n",
    "    \n",
    "* 2006-2020\n",
    "    * [hourly](https://www.ncei.noaa.gov/data/normals-hourly/2006-2020/archive/), [daily](https://www.ncei.noaa.gov/data/normals-daily/2006-2020/archive/), [monthly](https://www.ncei.noaa.gov/data/normals-monthly/2006-2020/archive/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ae24fa-f2b0-4b0d-a936-6032ac9df0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to grab the tar files for each of the normal variations:\n",
    "def get_tar_extract(main_url, normal_type, normal_period):\n",
    "    \n",
    "    normalFolder = f'{dataFolder}{normal_period}/{normal_type}/'\n",
    "    \n",
    "    if os.path.exists(normalFolder):\n",
    "        # print(f'{normal_period} {normal_type} already downloaded...')\n",
    "        return normalFolder\n",
    "        \n",
    "    base_url = f'{main_url}{normal_type}/{normal_period}/archive/'\n",
    "   \n",
    "    if normal_period == '1981-2010':\n",
    "        tar_url = f'{base_url}{normal_type}.tar.gz'\n",
    "    else:\n",
    "        html_page = req.urlopen(base_url)\n",
    "        soup = bsoup(html_page, \"html.parser\")\n",
    "        for link in soup.findAll('a'):\n",
    "            if 'station' in  link.text:\n",
    "                tar_url = f'{base_url}{link.text}'\n",
    "        \n",
    "    ftpstream = req.urlopen(tar_url)\n",
    "    file = tarfile.open(fileobj = ftpstream, mode = 'r|gz')\n",
    "    \n",
    "    print(f'Getting {normal_period} normals...')\n",
    "    print(f'*** {normal_type} extracting to {normalFolder} ***')\n",
    "    file.extractall(normalFolder)\n",
    "    print(f'*** {normal_type} extraction completed ***')\n",
    "    return normalFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create station inventory files for 1981-2010 dailys and monthlys normals. Returns station inventory file location to add to climates dict\n",
    "\n",
    "def get_station_inventory(normal_period, normal_type):\n",
    "    location = f'{climates[normal_period][normal_type][\"location\"]}'\n",
    "    inventory_csvName = f'{location}station-inventory.csv'\n",
    "    # climates[normal_period][normal_type]['inventory-file'] = inventory_csvName\n",
    "    \n",
    "    if os.path.exists(inventory_csvName):\n",
    "        # print(f'****** {inventory_csvName} exists... ******')\n",
    "        return inventory_csvName\n",
    "    \n",
    "    print(f'----- Creating {inventory_csvName}... -----')\n",
    "    df = pd.read_csv(f'{dataFolder}{period}/station-inventory.csv', dtype={'wmo_id': 'Int64'})\n",
    "    all_stations_list = list(df.station_id)\n",
    "    \n",
    "    station_list = []\n",
    "    for file in os.listdir(location):\n",
    "        station = file.split('.')[0]\n",
    "        station_list.append(all_stations_list.index(station))\n",
    "\n",
    "    inventory_df = df.iloc[station_list]\n",
    "    inventory_df.to_csv(inventory_csvName, index=False)\n",
    "    \n",
    "    return inventory_csvName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20dddc34-1e3e-44f6-a355-46d6f3b846c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_periods = ['1981-2010','1991-2020','2006-2020']\n",
    "normal_list = ['normals-hourly','normals-daily', 'normals-monthly']\n",
    "main_url = 'https://www.ncei.noaa.gov/data/'\n",
    "\n",
    "climates = {}\n",
    "for normal_period in normal_periods:\n",
    "    climates[normal_period] = {}\n",
    "    for normal_type in normal_list:\n",
    "        climates[normal_period][normal_type] = {'location' : get_tar_extract(main_url, normal_type, normal_period)}\n",
    "\n",
    "file_list =[]\n",
    "for normal_period in normal_periods:\n",
    "    for normal_type in normal_list:\n",
    "        inventory_file = get_station_inventory(normal_period, normal_type)\n",
    "        climates[normal_period][normal_type]['inventory-file'] = inventory_file\n",
    "        file_list.append(inventory_file)\n",
    "\n",
    "#creates massive csv file which combines all station inventory files\n",
    "mainInventory_file = f'./txt_files/all-stations.csv'        \n",
    "with open(mainInventory_file, 'w') as outfile:\n",
    "    outfile.write('station_id,latitude,longitude,elevation,state,name,network,wmo_id\\n')\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as infile:\n",
    "            infile.readline()\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "# opens massive csv file into pandas DataFrame to drop duplicate rows and then resaves csv file\n",
    "df = pd.read_csv(mainInventory_file)\n",
    "df = df.drop_duplicates(keep='last').sort_values(by=['state', 'name']).reset_index(drop=True)\n",
    "df.to_csv(mainInventory_file,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(db_Name):\n",
    "    try:\n",
    "        conn = psql.connect(\n",
    "                user=creds.USER,\n",
    "                password=creds.PASS,\n",
    "                host=creds.HOST,\n",
    "                port=creds.PORT,\n",
    "                database=db_Name\n",
    "                )\n",
    "    except psql.OperationalError as e:\n",
    "        print(f'There is no database named {db_Name}...')\n",
    "        create_db(db_Name)\n",
    "        return connect_db(db_Name)\n",
    "    else:\n",
    "        print(f'Connected: {db_Name}')\n",
    "        return conn\n",
    "        \n",
    "def create_db(db_Name):\n",
    "    conn = psql.connect(\n",
    "                user=creds.USER,\n",
    "                password=creds.PASS,\n",
    "                host=creds.HOST,\n",
    "                port=creds.PORT,\n",
    "                database='postgres'\n",
    "                )\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    sql=f'CREATE database {db_Name}'\n",
    "\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except psql.errors.lookup('42P04'): #psql error code for duplicatedatabase\n",
    "        conn.close()\n",
    "        return print(f\"....Database: {db_Name} already exists....\")\n",
    "    else:\n",
    "        print(f'....Database: {db_Name} created....')      \n",
    "        conn.close()\n",
    "        return print(f\"....Database: {db_Name} created successfully....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connected: climate_normals_db\n"
     ]
    }
   ],
   "source": [
    "a = connect_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "....Database: climate_normals_db already exists....\n"
     ]
    }
   ],
   "source": [
    "create_db('climate_normals_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates massive csv file which combines all normal files per type \n",
    "mainInventory_file = f'./txt_files/all-stations.csv'        \n",
    "with open(mainInventory_file, 'w') as outfile:\n",
    "    outfile.write('station_id,latitude,longitude,elevation,state,name,network,wmo_id\\n')\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as infile:\n",
    "            infile.readline()\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 1981-2010 headers\n",
    "#RRR-EEEE-SSSSSS[-CCCCCCC]#\n",
    "#RRR\n",
    "reporting_period = {\n",
    "    'ann': 'annual',\n",
    "    'djf': 'December, January, February',\n",
    "    'dly': 'daily',\n",
    "    'hly': 'hourly',\n",
    "    'jja': 'June, July, August',\n",
    "    'mam': 'March, April, May',\n",
    "    'mly': 'monthly',\n",
    "    'mtd': 'month-to-date',\n",
    "    'rtp': 'return periods',\n",
    "    'son': 'September, October, November',\n",
    "    'ytd': 'year-to-date'\n",
    "}\n",
    "#metorological element, EEEE\n",
    "met_elem = {\n",
    "    'cldd': 'cooling degree days',\n",
    "    'cldh': 'cooling degree hours',\n",
    "    'clod': 'clouds',\n",
    "    'dewp': 'dew point temperature',\n",
    "    'dutr': 'diurnal temperature range',\n",
    "    'hidx': 'heat index',\n",
    "    'htdd': 'heating degree days',\n",
    "    'htdh': 'heating degree hours',\n",
    "    'prcp': 'precipitation',\n",
    "    'pres': 'sea level pressure',\n",
    "    'snow': 'snowfall',\n",
    "    'snwd': 'snow depth',\n",
    "    'tavg': 'daily mean temperature (average of tmax and tmin)',\n",
    "    'temp': 'temperature',\n",
    "    'tmax': 'daily maximum temperature',\n",
    "    'tmin': 'daily minimum temperature',\n",
    "    'wchl': 'wind chill',\n",
    "    'wind': 'wind'\n",
    "}\n",
    "\n",
    "#Statistic, SSSSSS\n",
    "statistic = {\n",
    "    '10pctl': 'Climatological 10th percentile',\n",
    "    '1stdir': 'Prevailing Wind Direction',\n",
    "    '1stpct': 'Prevailing Wind Percentage',\n",
    "    '2nddir': 'Secondary Wind Direction',\n",
    "    '2ndpct': 'Secondary Wind Percentage',\n",
    "    '25pctl': 'Climatological 25th percentile',\n",
    "    '50pctl': 'Climatological 50th percentile',\n",
    "    '75pctl': 'Climatological 75th percentile',\n",
    "    '90pctl': 'Climatological 90th percentile',\n",
    "    'avgnds': 'Average Number of Days (followed by a condition)',\n",
    "    'avgspd': 'Average Wind Speed',\n",
    "    'baseNN': 'Average of base NN (other than 65F) Heating or Cooling Degree Days\n",
    "    'normal': 'Climatological Average',\n",
    "    'pctall': 'Probability of Occurrence (followed by a condition)',\n",
    "    'pctbkn': 'Percent Broken (clouds)',\n",
    "    'pctclm': 'Percent Calm (winds)',\n",
    "    'pctclr': 'Percent Clear (clouds)',\n",
    "    'pctfew': 'Percent Few (clouds)',\n",
    "    'pctovc': 'Percent Overcast (clouds)',\n",
    "    'pctsct': 'Percent Scattered (clouds)',\n",
    "    'vctdir': 'Mean Wind Vector Direction',\n",
    "    'vctspd': 'Mean Wind Vector Magnitude'\n",
    "}\n",
    "\n",
    "#Condition, -CCCCCCC\n",
    "condition = {\n",
    "    'geNNNhi' : 'greater than or equal to NNN hundredths of inches NNN can be 001,010,050,100 (for precipitation)', \n",
    "    'geNNNti' : 'greater than or equal to NNN tenths of inches NNN can be 001,010,030,050,100 (for snowfall)',\n",
    "    'geNNNwi' : 'greater than or equal to NNN whole inches NNN can be 001,003,005,010 (for snow depth)',\n",
    "    'grthNNN' : 'greater than or equal to NNN whole degrees Fahrenheit NNN can be 040,050,060,070,080,090,100',\n",
    "    'lsthNNN' : 'less than or equal to NNN whole degrees Fahrenheit NNN can be 000,010,020,032,040,050,060'\n",
    "}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd08829f2f28fb4fe9df99e8c305ffe937ba372ff45bcdf063cdbd318f5788220f1",
   "display_name": "Python 3.9.2 64-bit ('Climate-Normals': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}